{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Thrupthi Ann John https://github.com/ThrupthiAnn\n",
    "\n",
    "\n",
    "# Demo of CIS and CMS maps\n",
    "\n",
    "This notebook takes you through the steps to calculate Canonical Image Saliency maps and Model Saliency Maps. \n",
    " In our paper, the experiments are conducted for 22085 random images from the CelebA dataset. In this demo, we have provided data for 100 images. Feel free to use your own images. You can use any model in PyTorch, although we have provided models for VGG-16 (recognition and gender) trained on CelebA. \n",
    "\n",
    "<p>This is the first demo notebook. After this, run <b>demo2_explanation.ipynb</b> and then <b>demo3_metrics.ipynb</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folder names and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from scipy.io import loadmat\n",
    "from CalculateCIS import Occlusion\n",
    "import matplotlib.colors\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from os import makedirs\n",
    "\n",
    "samplefolder = '../data/SampleData'\n",
    "imagefolder = join(samplefolder, 'SampleImages')\n",
    "meshfolder = join(samplefolder, 'Sample3D')\n",
    "\n",
    "resultsfolder = '../results'\n",
    "cisfolder = join(resultsfolder, 'CIS')\n",
    "\n",
    "makedirs(cisfolder, exist_ok=True)\n",
    "modelfolder = '../data/Models'\n",
    "frontal = imread(join(modelfolder, 'frontal.jpg'))\n",
    "frontal_mesh = loadmat(join(modelfolder, 'frontal_mesh.mat'))['vertices'][::20,:]\n",
    "size = 15\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PlotColormap(colormap, alpha=0.7):\n",
    "\talphas = np.ones(colormap.shape)\n",
    "\talphas[colormap==0] = 0\n",
    "\tvmin = np.min(colormap)\n",
    "\tvmax = np.max(colormap)\n",
    "\tcmap = plt.cm.jet\n",
    "\tcolors = matplotlib.colors.Normalize(vmin, vmax, clip=True)(colormap)\n",
    "\tcolors = cmap(colors)\n",
    "\tcolors[..., -1] = alphas*alpha\n",
    "\tfig = plt.figure()\n",
    "\tplt.imshow(frontal)\n",
    "\tplt.imshow(colors)\n",
    "\tplt.axis('off')\n",
    "\tplt.colorbar()\n",
    "\t\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "Norm = transforms.Compose([transforms.ToTensor(),\n",
    "\t\t\t\t\t\t   transforms.Normalize(mean, std, inplace=True)])\n",
    "\n",
    "def ViewImage(filename):\n",
    "\timg = imread(filename)\n",
    "\tfig = plt.figure()\n",
    "\tplt.imshow(img)\n",
    "\tplt.show()\n",
    "\t\n",
    "\n",
    "def preprocess_image(pil_im, resize_im=True):\n",
    "    \"\"\"\n",
    "        Processes image for CNNs\n",
    "\n",
    "    Args:\n",
    "        PIL_img (PIL_img): Image to process\n",
    "        resize_im (bool): Resize to 224 or not\n",
    "    returns:\n",
    "        im_as_var (torch variable): Variable that contains processed float tensor\n",
    "    \"\"\"\n",
    "    # mean and std list for channels (Imagenet)\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    # Resize image\n",
    "    if resize_im:\n",
    "        pil_im.thumbnail((512, 512))\n",
    "    im_as_arr = np.float32(pil_im)\n",
    "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
    "    # Normalize the channels\n",
    "    for channel, _ in enumerate(im_as_arr):\n",
    "        im_as_arr[channel] /= 255\n",
    "        im_as_arr[channel] -= mean[channel]\n",
    "        im_as_arr[channel] /= std[channel]\n",
    "    # Convert to float tensor\n",
    "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
    "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
    "    im_as_ten.unsqueeze_(0)\n",
    "    # Convert to Pytorch variable\n",
    "   # im_as_var = Variable(im_as_ten, requires_grad=True)\n",
    "    im_as_ten = im_as_ten.to(device);\n",
    "    im_as_ten.requires_grad=True;\n",
    "    return im_as_ten\n",
    "\t\n",
    "def get_image(filename):\n",
    "    img = Image.open(filename)\n",
    "    orig_size = img.size\n",
    "    img = img.resize((224,224))\n",
    "    pimg= preprocess_image(img)\n",
    "    return pimg, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get test images\n",
    "Put your test images in ./SampleImages. 100 sample images are provided. \n",
    "\n",
    "## Step 2: Obtain the 3D dense alignment of the sample images.\n",
    "The 3D alignment of the images in ./SampleImages are provided for you in ./Sample3D. <br>\n",
    "If you want to run for your own images, clone the repository https://github.com/YadiraF/PRNet You need Tensorflow for this. \n",
    "\n",
    "Run the following command:\n",
    "\n",
    "    python PRNet/demo.py -i SampleImages -o Sample3D --isMat True\n",
    "\n",
    "<p/>\n",
    "There is a pytorch version at https://github.com/tomguluson92/PRNet_PyTorch , although I have not tested it. \n",
    "    \n",
    "    \n",
    "## Step 3: Calculate the CIS map of all sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide code for VGG-Face trained on CelebA. If you want to use another model, please write your own version of the function <b>Confidence(image, classid)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Confidence(image, class_index=None):\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(image)\n",
    "\tif class_index is None:\n",
    "\t\tclass_index = torch.argmax(output)\n",
    "\treturn output[:,class_index].detach(), class_index\n",
    "\n",
    "def loadVGGModel( filename):\n",
    "\tdat2 = torch.load(filename)\n",
    "\t# copy dictionary\n",
    "\tif str.split(list(dat2.keys())[0],'.')[0] == 'module':\n",
    "\t\tdat = {}\n",
    "\t\tfor key in dat2.keys():\n",
    "\t\t\tk = '.'.join(str.split(key,'.')[1:])\n",
    "\t\t\tdat[k] = dat2[key]\n",
    "\telse:\n",
    "\t\tdat = dat2\n",
    "\t\t\n",
    "\tn_classes = dat['classifier.6.bias'].shape[0]\n",
    "\tmodel = models.vgg16(pretrained = False)\n",
    "\tlastlayer = torch.nn.Linear(in_features = model.classifier[-1].in_features, \\\n",
    "\t\t\t\t\t\t\t   out_features = n_classes, \\\n",
    "\t\t\t\t\t\t\t   bias = True)\n",
    "\tmodel.classifier[-1] = lastlayer\n",
    "\tmodel.load_state_dict(dat)\n",
    "\treturn model\n",
    "\n",
    "# model = loadVGGModel(join(modelfolder, 'VGG16_CelebA_Gender.pth'))\n",
    "model = loadVGGModel(join(modelfolder, 'VGG16_CelebA_Recognition.pth')) # here is the recognition model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get list of files\n",
    "p = Path(imagefolder)\n",
    "filenames = [i.stem for i in p.glob('**/*.jpg')]\n",
    "\n",
    "# get CIS map for each image. This takes some time. \n",
    "for ii in range(len(filenames)):\n",
    "\t# print(ii)\n",
    "\timg, sz = get_image(join(imagefolder,filenames[ii]+'.jpg'))\n",
    "\tmesh = loadmat(join(meshfolder, filenames[ii])+'_mesh.mat')['vertices']\n",
    "\t# subsample the mesh to make the calculation faster\n",
    "\tif len(mesh)>2194:\n",
    "\t\tmesh = mesh[::20,:]\n",
    "\theatmap = Occlusion(Confidence, img.to(device, dtype = torch.float), mesh, sz, frontal, frontal_mesh, size = size,class_index=None, device = device);\n",
    "\toutfilename = join(cisfolder, filenames[ii])\n",
    "\tnp.save(outfilename, heatmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the CIS maps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(np.load(join(cisfolder, '000686.npy')), cmap = 'jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate the CMS maps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Path(cisfolder)\n",
    "filenames = [i.stem for i in p.glob('**/*.npy')]\n",
    "heatmap = []\n",
    "for ii in range(len(filenames)):\n",
    "\thmap = np.load(join(cisfolder, filenames[ii]+'.npy'))\n",
    "\tif len(heatmap)==0:\n",
    "\t\theatmap = hmap\n",
    "\telse:\n",
    "\t\theatmap = heatmap+hmap\n",
    "\t\t\n",
    "PlotColormap(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
