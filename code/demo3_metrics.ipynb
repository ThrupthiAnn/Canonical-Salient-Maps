{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c19730",
   "metadata": {},
   "source": [
    "Author: Thrupthi Ann John https://github.com/ThrupthiAnn\n",
    "\n",
    "# Demo of calculating the metrics in our paper\n",
    "This is the last demo notebook. Before running this notebook, please run <b>demo1_maps.ipynb</b> and <b>demo2_explanation.ipynb</b>. In this notebook, we will find the confidence of the images before and after creating explanation maps and run the metrics given in our paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c3f9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from os.path import join\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "datafolder = '../data'\n",
    "imagefolder = join(datafolder, 'SampleData/SampleImages')\n",
    "modelfolder = join(datafolder, 'Models')\n",
    "interfolder = '../results/IntermediateResults'\n",
    "explcmsfolder = join(interfolder, 'Explanation_CMS')\n",
    "explgradcamfolder = join(interfolder, 'Explanation_GradCAM')\n",
    "explgradcamplusfolder = join(interfolder, 'Explanation_GradCAMPlus')\n",
    "explscorecamfolder = join(interfolder, 'Explanation_ScoreCAM')\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "p = Path(imagefolder)\n",
    "filenames = [i.stem + '.jpg' for i in p.glob('**/*')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327ef79",
   "metadata": {},
   "source": [
    "# Step 1: Initialize.\n",
    "\n",
    "## Initialize the deep model\n",
    "Here, we provide code for VGG-Face trained on CelebA. Please write your own model loading function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba891ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10177, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadVGGModel( filename):\n",
    "\tdat2 = torch.load(filename)\n",
    "\t# copy dictionary\n",
    "\tif str.split(list(dat2.keys())[0],'.')[0] == 'module':\n",
    "\t\tdat = {}\n",
    "\t\tfor key in dat2.keys():\n",
    "\t\t\tk = '.'.join(str.split(key,'.')[1:])\n",
    "\t\t\tdat[k] = dat2[key]\n",
    "\telse:\n",
    "\t\tdat = dat2\n",
    "\t\t\n",
    "\tn_classes = dat['classifier.6.bias'].shape[0]\n",
    "\tmodel = models.vgg16(pretrained = False)\n",
    "\tlastlayer = torch.nn.Linear(in_features = model.classifier[-1].in_features, \\\n",
    "\t\t\t\t\t\t\t   out_features = n_classes, \\\n",
    "\t\t\t\t\t\t\t   bias = True)\n",
    "\tmodel.classifier[-1] = lastlayer\n",
    "\tmodel.load_state_dict(dat)\n",
    "\treturn model\n",
    "\n",
    "#model = loadVGGModel('VGG16_CelebA_Gender.pth')\n",
    "model = loadVGGModel(join(modelfolder,'VGG16_CelebA_Recognition.pth')) # here is the recognition model\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8686ae50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(pil_im, resize_im=True):\n",
    "    \"\"\"\n",
    "        Processes image for CNNs\n",
    "\n",
    "    Args:\n",
    "        PIL_img (PIL_img): Image to process\n",
    "        resize_im (bool): Resize to 224 or not\n",
    "    returns:\n",
    "        im_as_var (torch variable): Variable that contains processed float tensor\n",
    "    \"\"\"\n",
    "    # mean and std list for channels (Imagenet)\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    # Resize image\n",
    "    if resize_im:\n",
    "        pil_im.thumbnail((512, 512))\n",
    "    im_as_arr = np.float32(pil_im)\n",
    "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
    "    # Normalize the channels\n",
    "    for channel, _ in enumerate(im_as_arr):\n",
    "        im_as_arr[channel] /= 255\n",
    "        im_as_arr[channel] -= mean[channel]\n",
    "        im_as_arr[channel] /= std[channel]\n",
    "    # Convert to float tensor\n",
    "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
    "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
    "    # Convert to Pytorch variable\n",
    "   # im_as_var = Variable(im_as_ten, requires_grad=True)\n",
    "    im_as_ten = im_as_ten.to(device);\n",
    "    im_as_ten.requires_grad=True;\n",
    "    return im_as_ten\n",
    "\t\n",
    "def get_image(filename):\n",
    "    img = Image.open(filename)\n",
    "    orig_size = img.size\n",
    "    img = img.resize((224,224))\n",
    "    pimg= preprocess_image(img)\n",
    "    return pimg, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99657a7",
   "metadata": {},
   "source": [
    "## Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437286b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, datafolder, filelist):\n",
    "\t\tself.datafolder = datafolder\n",
    "\t\tself.filelist = filelist\n",
    "\t\tself.length = len(self.filelist)\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.length\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn get_image(join(self.datafolder, self.filelist[index]))[0], self.filelist[index]\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967088d",
   "metadata": {},
   "source": [
    "# Step 2: Get the Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38497a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confidence(dataset):\n",
    "\tdataloader = torch.utils.data.dataloader.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\t# create variables to store score and class\n",
    "\tscore = np.arange(len(dataset)).astype(np.float32)\n",
    "\tindex = np.arange(len(dataset))\n",
    "\tfilelist = []\n",
    "\tlastind = 0\n",
    "\tfor ii, data in enumerate(dataloader):\n",
    "\t\timg, file = data\n",
    "\t\tfilelist.extend(file)\n",
    "\t\tprint('\\r%s'%ii,end='           ')\n",
    "\t\toutputs = model(img.cuda()).detach()\n",
    "\t\tval, ind = torch.max(outputs,1)\n",
    "\t\tscore[lastind:lastind+len(val)] = val.cpu().numpy()\n",
    "\t\tindex[lastind:lastind+len(ind)] = ind.cpu().numpy()\n",
    "\t\tlastind = lastind+len(val)\n",
    "\t\t\n",
    "\treturn score, index, filelist\n",
    "\n",
    "def confidence_for_class(dataset, classes):\n",
    "\tdataloader = torch.utils.data.dataloader.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\t# create variables to store score and class\n",
    "\tscore = np.arange(len(dataset)).astype(np.float32)\n",
    "\tclasses = classes.flatten()\n",
    "\tlastind = 0\n",
    "\tfor ii, data in enumerate(dataloader):\n",
    "\t\tprint('\\r%s'%ii,end='           ')\n",
    "\t\timg, filename = data\n",
    "\t\toutputs = model(img.cuda()).detach().cpu().numpy()\n",
    "\t\tfor jj in range(outputs.shape[0]):\n",
    "\t\t\tscore[lastind:lastind+jj]=outputs[jj][classes[lastind+jj]]\n",
    "\t\tlastind = lastind+outputs.shape[0]\n",
    "\t\t\n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e0f92d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3           "
     ]
    }
   ],
   "source": [
    "# score on unaltered images\n",
    "images = Dataset(imagefolder, filenames)\n",
    "imagescore, index, filelist = confidence(images)\n",
    "\n",
    "# create other datasets with the same file order\n",
    "gradcam = Dataset(explgradcamfolder, filelist)\n",
    "gradcamplus = Dataset(explgradcamplusfolder, filelist)\n",
    "scorecam = Dataset(explscorecamfolder, filelist)\n",
    "cms = Dataset(explcmsfolder, filelist)\n",
    "\n",
    "# get scores for the original classes\n",
    "gradcamscore = confidence_for_class(gradcam, index)\n",
    "gradcamplusscore = confidence_for_class(gradcamplus, index)\n",
    "scorecamscore = confidence_for_class(scorecam, index)\n",
    "cmsscore = confidence_for_class(cms, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136c1b1",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7486ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AverageDrop(fullscore, explscore):\n",
    "\treturn  np.sum(np.maximum(0, fullscore-explscore)/fullscore)*100/len(fullscore)\n",
    "\n",
    "def IncreaseConfidence(fullscore, explscore):\n",
    "\treturn np.sum(fullscore<explscore)/len(fullscore)*100\n",
    "\n",
    "def Win(gcscore, gcpscore, scscore, trscore):\n",
    "\t# for each, check which one is the hightest\n",
    "\t\n",
    "\tmaxscores = np.argmin(np.vstack((gcscore,gcpscore, scscore, trscore)),axis=0)\n",
    "\tgc = np.sum(maxscores==0)\n",
    "\tgcp = np.sum(maxscores==1)\n",
    "\tsc = np.sum(maxscores==2)\n",
    "\ttr = np.sum(maxscores==3)\n",
    "\tlength = len(gcscore)\n",
    "\t\n",
    "\tprint('GradCAM\\t\\t:\\t', gc/length*100)\n",
    "\tprint('GradCAM++\\t:\\t', gcp/length*100)\n",
    "\tprint('ScoreCAM\\t:\\t', sc/length*100)\n",
    "\tprint('CMS\\t\\t:\\t', tr/length*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eae7f6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Drop: (Higher is better)\n",
      "GradCAM\t\t:\t 37.9015998840332\n",
      "GradCAM++\t:\t 42.49958419799805\n",
      "ScoreCAM\t:\t 39.13896942138672\n",
      "CMS\t\t:\t 65.07044219970703\n",
      "\n",
      "% Increase in confidence: (Lower is better)\n",
      "GradCAM\t\t:\t 8.0\n",
      "GradCAM++\t:\t 7.000000000000001\n",
      "ScoreCAM\t:\t 10.0\n",
      "CMS\t\t:\t 4.0\n",
      "\n",
      "Win %: (Higher is better)\n",
      "GradCAM\t\t:\t 4.0\n",
      "GradCAM++\t:\t 0.0\n",
      "ScoreCAM\t:\t 0.0\n",
      "CMS\t\t:\t 96.0\n"
     ]
    }
   ],
   "source": [
    "print('Average Drop: (Higher is better)')\n",
    "print('GradCAM\\t\\t:\\t', AverageDrop(imagescore, gradcamscore))\n",
    "print('GradCAM++\\t:\\t', AverageDrop(imagescore, gradcamplusscore))\n",
    "print('ScoreCAM\\t:\\t', AverageDrop(imagescore, scorecamscore))\n",
    "print('CMS\\t\\t:\\t', AverageDrop(imagescore, cmsscore))\n",
    "print('\\n% Increase in confidence: (Lower is better)')\n",
    "print('GradCAM\\t\\t:\\t', IncreaseConfidence(imagescore, gradcamscore))\n",
    "print('GradCAM++\\t:\\t', IncreaseConfidence(imagescore, gradcamplusscore))\n",
    "print('ScoreCAM\\t:\\t', IncreaseConfidence(imagescore, scorecamscore))\n",
    "print('CMS\\t\\t:\\t', IncreaseConfidence(imagescore, cmsscore))\n",
    "print('\\nWin %: (Higher is better)')\n",
    "Win(gradcamscore, gradcamplusscore, scorecamscore, cmsscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a8a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a83966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
